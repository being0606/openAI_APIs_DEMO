{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall numpy -y \n",
    "# !conda install -c conda-forge numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making API request data: 100%|██████████| 10/10 [00:00<00:00, 295373.52it/s]\n",
      "INFO:__main__:Total 10 API requests are made!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_id': '1427_10_17_2', 'method': 'POST', 'url': '/v1/chat/completions', 'body': {'model': 'gpt-4o-mini', 'messages': [{'role': 'system', 'content': 'You are an expert in translating Hanja documents into English.\\nThe documents are written in Joseon, which is one of the Korean dynasties.\\nTranslate the input document into English with the following compact JSON format:\\n{\"translated\": {The translated document}}'}, {'role': 'user', 'content': '○右司諫金孝貞等上疏曰: 竊謂凡所施爲, 視歲豐歉。 比年以來, 水旱相仍, 禾稼不稔, 而今年正値農月, 旱乾爲甚, 殿下(霄) 旰軫念, 救荒恤民之政, 無不畢擧, 德至渥也。 今以忠淸、慶尙、全羅、江原、咸吉等道農事爲優, 許遣軍容敬差官, 將點兵船軍器, 以備不虞, 慮至深也。 然上項各道, 雖不若京畿、黃海、平安道之爲歉, 亦恐未至於豐稔也。 且他道之飢餓者, 將或轉而求食, 及其終也, 土着之民, 不免艱食之憂, 勢所必至, 其於點閱之際, 搔擾人民, 又非一端, 弊固不小。 伏望殿下姑停遣官, 待後豐年, 點考施行, 以慰民生。 上議于政府, 命停之。'}], 'max_tokens': 1000, 'temperature': 0.7, 'seed': 42}}\n"
     ]
    }
   ],
   "source": [
    "# 로거 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 시스템 프롬프트 정의\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert in translating Hanja documents into English.\n",
    "The documents are written in Joseon, which is one of the Korean dynasties.\n",
    "Translate the input document into English with the following compact JSON format:\n",
    "{\"translated\": {The translated document}}\n",
    "\"\"\".strip()\n",
    "\n",
    "# 이전에 생성한 데이터프레임 로드 (필요에 따라 경로 수정)\n",
    "df_sentences_filtered = pd.read_csv(\"../data/sentencePair_세종실록_filtered.csv\")\n",
    "df_sentences_filtered_sample = df_sentences_filtered.sample(10, random_state=42)\n",
    "\n",
    "\n",
    "# target_data 생성\n",
    "target_data = df_sentences_filtered_sample.to_dict('records')\n",
    "\n",
    "# args 생성\n",
    "args = Namespace(\n",
    "    model='gpt-4o-mini',  # 사용할 모델명\n",
    "    max_completion_tokens=1000,\n",
    "    temperature=0.7,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# make_api_request_data 함수 정의\n",
    "def make_api_request_data(target_data: List[Dict[str, Any]], args: Namespace) -> List[Dict[str, Any]]:\n",
    "    # https://platform.openai.com/docs/guides/batch/getting-started 참조\n",
    "    request_data = []\n",
    "\n",
    "    for data_idx, datum in enumerate(tqdm(target_data, desc=\"Making API request data\", mininterval=1)):\n",
    "        # custom_id 생성 (year, month, day, sentence_index 조합)\n",
    "        custom_id = f\"{datum['year']}_{datum['month']}_{datum['day']}_{datum['sentence_index']}\"\n",
    "\n",
    "        request_data.append(\n",
    "            {\n",
    "                \"custom_id\": custom_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": args.model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                        {\"role\": \"user\", \"content\": datum[\"hanja\"]},\n",
    "                    ],\n",
    "                    \"max_tokens\": args.max_completion_tokens,\n",
    "                    \"temperature\": args.temperature,\n",
    "                    \"seed\": args.seed\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    logger.info(f\"Total {len(request_data)} API requests are made!\")\n",
    "    return request_data\n",
    "\n",
    "# 함수 호출하여 API 요청 데이터 생성\n",
    "api_requests = make_api_request_data(target_data, args)\n",
    "\n",
    "# 결과 확인 (예시로 첫 번째 요청 데이터 출력)\n",
    "print(api_requests[0])\n",
    "\n",
    "# \"../data/ch02_batchinput.jsonl\", \"rb\" 해당경로에 저장\n",
    "with open(\"../data/ch03_batchinput_sentencePair_세종실록_filtered_sampled_10.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for request in api_requests:\n",
    "        f.write(json.dumps(request, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: batch_67443bc16cf08190a7246b9cb492ce01\n",
      "Status: validating\n",
      "Created At: 1732524993\n",
      "input_file_id: file-JVgBMZfhVEAzUnFXwHsAnw\n",
      "output_file_id: None\n",
      "------\n",
      "Batch ID: batch_674338f7552881908768f70dae8e5ea9\n",
      "Status: completed\n",
      "Created At: 1732458743\n",
      "input_file_id: file-Q8mYsoemnm5DxZzjiPHUZY\n",
      "output_file_id: file-PVsQEdu7xe2Qe1uHSzEUXQ\n",
      "------\n",
      "Batch ID: batch_674336d52b688190b0d397478747745f\n",
      "Status: completed\n",
      "Created At: 1732458197\n",
      "input_file_id: file-Cj8Lt9CWeyuJZ925C2ktKF\n",
      "output_file_id: file-7ztiejaK3fWFWvfVBxZqUg\n",
      "------\n",
      "Batch ID: batch_674335e4d4ac81909430e1e4f05dc8a3\n",
      "Status: completed\n",
      "Created At: 1732457956\n",
      "input_file_id: file-L88QzYT6zQbzhDtsH8GCJa\n",
      "output_file_id: file-T2RKu1GkBaRZvsbmKPZMDM\n",
      "------\n",
      "Batch ID: batch_6742fb58f76c8190b8241508f9b54704\n",
      "Status: completed\n",
      "Created At: 1732442968\n",
      "input_file_id: file-FHYHLzZog8azWz7UH8e917\n",
      "output_file_id: file-EA5oLpKAtv5vA2SEnSdnhX\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  17:56:35    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  17:57:35    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  17:58:36    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  17:59:37    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:00:37    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:01:38    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:02:39    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:03:39    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:04:40    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:05:41    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:06:42    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:07:43    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:08:44    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:09:44    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:10:45    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:11:46    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:12:47    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:13:48    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:14:49    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:15:49    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:16:50    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:17:51    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:18:52    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:19:53    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:20:53    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:21:54    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:22:55    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:23:56    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:24:57    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:25:57    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:26:58    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:27:59    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:29:00    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:30:00    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:31:01    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:32:02    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:33:03    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:34:03    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:35:04    |    status:  in_progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/batches/batch_67443bc16cf08190a7246b9cb492ce01 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  18:36:05    |    status:  in_progress\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m   \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 60초에 한번씩 확인\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time: \u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# output file을 다운로드 받아서 출력\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# file upload\n",
    "batch_input_file = client.files.create(\n",
    "  file=open(\"../data/ch03_batchinput_sentencePair_세종실록_filtered_sampled_10.jsonl\", \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "\n",
    "# Create a new batch\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"테스트를 위한 배치\",\n",
    "    }\n",
    ")\n",
    "start_time = time.time()\n",
    "\n",
    "# Batch 객체들의 리스트를 가져옵니다.\n",
    "batches = client.batches.list().data\n",
    "\n",
    "# 각 배치의 상세 정보를 출력합니다.\n",
    "for batch in batches[:5]:\n",
    "    print(f\"Batch ID: {batch.id}\")\n",
    "    print(f\"Status: {batch.status}\")\n",
    "    print(f\"Created At: {batch.created_at}\")\n",
    "    print(f\"input_file_id: {batch.input_file_id}\")\n",
    "    print(f\"output_file_id: {batch.output_file_id}\")\n",
    "    print(\"------\")\n",
    "    \n",
    "\n",
    "# status를 60초에 한번씩 확인하고 'completed'일때까지 로그를 출력\n",
    "# status가 'completed'가 되면 루프를 탈출합니다.\n",
    "# 총 걸린 시간을 추가로 출력합니다.\n",
    "while True:\n",
    "  status = client.batches.retrieve(client.batches.list().data[0].id).status\n",
    "  print(\"time: \", time.strftime('%X', time.localtime()), \"   |    status: \", status)\n",
    "  if status == \"completed\":\n",
    "    break\n",
    "  time.sleep(60) # 60초에 한번씩 확인\n",
    "\n",
    "print(\"Total time: \", time.time() - start_time)\n",
    "\n",
    "\n",
    "# output file을 다운로드 받아서 출력\n",
    "file_response = client.files.content(client.batches.list().data[0].output_file_id).text.encode('utf-8').decode('unicode-escape')\n",
    "print(file_response)\n",
    "\n",
    "file_response[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: batch_67443bc16cf08190a7246b9cb492ce01\n",
      "Status: in_progress\n",
      "Created At: 1732524993\n",
      "input_file_id: file-JVgBMZfhVEAzUnFXwHsAnw\n",
      "output_file_id: None\n",
      "------\n",
      "Batch ID: batch_674338f7552881908768f70dae8e5ea9\n",
      "Status: completed\n",
      "Created At: 1732458743\n",
      "input_file_id: file-Q8mYsoemnm5DxZzjiPHUZY\n",
      "output_file_id: file-PVsQEdu7xe2Qe1uHSzEUXQ\n",
      "------\n",
      "Batch ID: batch_674336d52b688190b0d397478747745f\n",
      "Status: completed\n",
      "Created At: 1732458197\n",
      "input_file_id: file-Cj8Lt9CWeyuJZ925C2ktKF\n",
      "output_file_id: file-7ztiejaK3fWFWvfVBxZqUg\n",
      "------\n",
      "Batch ID: batch_674335e4d4ac81909430e1e4f05dc8a3\n",
      "Status: completed\n",
      "Created At: 1732457956\n",
      "input_file_id: file-L88QzYT6zQbzhDtsH8GCJa\n",
      "output_file_id: file-T2RKu1GkBaRZvsbmKPZMDM\n",
      "------\n",
      "Batch ID: batch_6742fb58f76c8190b8241508f9b54704\n",
      "Status: completed\n",
      "Created At: 1732442968\n",
      "input_file_id: file-FHYHLzZog8azWz7UH8e917\n",
      "output_file_id: file-EA5oLpKAtv5vA2SEnSdnhX\n",
      "------\n",
      "time:  19:01:21    |    status:  in_progress\n",
      "time:  19:02:22    |    status:  in_progress\n",
      "time:  19:03:22    |    status:  in_progress\n",
      "time:  19:04:23    |    status:  in_progress\n",
      "time:  19:05:24    |    status:  in_progress\n",
      "time:  19:06:24    |    status:  in_progress\n",
      "time:  19:07:25    |    status:  in_progress\n",
      "time:  19:08:26    |    status:  in_progress\n",
      "time:  19:09:26    |    status:  in_progress\n",
      "time:  19:10:27    |    status:  in_progress\n",
      "time:  19:11:28    |    status:  in_progress\n",
      "time:  19:12:29    |    status:  in_progress\n",
      "time:  19:13:30    |    status:  in_progress\n",
      "time:  19:14:30    |    status:  in_progress\n",
      "time:  19:15:31    |    status:  in_progress\n",
      "time:  19:16:32    |    status:  in_progress\n",
      "time:  19:17:32    |    status:  in_progress\n",
      "time:  19:18:33    |    status:  in_progress\n",
      "time:  19:19:34    |    status:  in_progress\n",
      "time:  19:20:35    |    status:  in_progress\n",
      "time:  19:21:35    |    status:  in_progress\n",
      "time:  19:22:36    |    status:  in_progress\n",
      "time:  19:23:36    |    status:  in_progress\n",
      "time:  19:24:38    |    status:  in_progress\n",
      "time:  19:25:41    |    status:  in_progress\n",
      "time:  19:26:42    |    status:  in_progress\n",
      "time:  19:27:43    |    status:  in_progress\n",
      "time:  19:28:43    |    status:  in_progress\n",
      "time:  19:29:44    |    status:  in_progress\n",
      "time:  19:30:45    |    status:  in_progress\n",
      "time:  19:31:45    |    status:  in_progress\n",
      "time:  19:32:46    |    status:  in_progress\n",
      "time:  19:33:47    |    status:  in_progress\n",
      "time:  19:34:47    |    status:  in_progress\n",
      "time:  19:35:48    |    status:  in_progress\n",
      "time:  19:36:48    |    status:  in_progress\n",
      "time:  19:37:49    |    status:  in_progress\n",
      "time:  19:38:50    |    status:  in_progress\n",
      "time:  19:39:50    |    status:  in_progress\n",
      "time:  19:40:51    |    status:  in_progress\n",
      "time:  19:41:52    |    status:  in_progress\n",
      "time:  19:42:52    |    status:  in_progress\n",
      "time:  19:43:53    |    status:  in_progress\n",
      "time:  19:44:54    |    status:  in_progress\n",
      "time:  19:45:54    |    status:  in_progress\n",
      "time:  19:46:55    |    status:  in_progress\n",
      "time:  19:47:56    |    status:  in_progress\n",
      "time:  19:48:56    |    status:  in_progress\n",
      "time:  19:49:57    |    status:  in_progress\n",
      "time:  19:50:58    |    status:  in_progress\n",
      "time:  19:51:58    |    status:  in_progress\n",
      "time:  19:52:59    |    status:  in_progress\n",
      "time:  19:53:59    |    status:  in_progress\n",
      "time:  19:55:00    |    status:  in_progress\n",
      "time:  19:56:19    |    status:  in_progress\n",
      "time:  19:59:39    |    status:  in_progress\n",
      "time:  20:00:40    |    status:  in_progress\n",
      "time:  20:01:41    |    status:  in_progress\n",
      "time:  20:02:41    |    status:  in_progress\n",
      "time:  20:03:42    |    status:  in_progress\n",
      "time:  20:04:42    |    status:  in_progress\n",
      "time:  20:05:43    |    status:  in_progress\n",
      "time:  20:06:43    |    status:  in_progress\n",
      "time:  20:07:44    |    status:  in_progress\n",
      "time:  20:08:45    |    status:  in_progress\n",
      "time:  20:09:45    |    status:  in_progress\n",
      "time:  20:10:46    |    status:  in_progress\n",
      "time:  20:11:47    |    status:  in_progress\n",
      "time:  20:12:47    |    status:  in_progress\n",
      "time:  20:13:48    |    status:  in_progress\n",
      "time:  20:14:48    |    status:  in_progress\n",
      "time:  20:15:49    |    status:  in_progress\n",
      "time:  20:16:50    |    status:  in_progress\n",
      "time:  20:17:50    |    status:  in_progress\n",
      "time:  20:18:51    |    status:  in_progress\n",
      "time:  20:19:52    |    status:  in_progress\n",
      "time:  20:20:52    |    status:  in_progress\n",
      "time:  20:21:53    |    status:  in_progress\n",
      "time:  20:22:54    |    status:  in_progress\n",
      "time:  20:23:54    |    status:  in_progress\n",
      "time:  20:24:55    |    status:  in_progress\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m   \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 60초에 한번씩 확인\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time: \u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# output file을 다운로드 받아서 출력\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Batch 객체들의 리스트를 가져옵니다.\n",
    "batches = client.batches.list().data\n",
    "\n",
    "# 각 배치의 상세 정보를 출력합니다.\n",
    "for batch in batches[:5]:\n",
    "    print(f\"Batch ID: {batch.id}\")\n",
    "    print(f\"Status: {batch.status}\")\n",
    "    print(f\"Created At: {batch.created_at}\")\n",
    "    print(f\"input_file_id: {batch.input_file_id}\")\n",
    "    print(f\"output_file_id: {batch.output_file_id}\")\n",
    "    print(\"------\")\n",
    "    \n",
    "\n",
    "# status를 60초에 한번씩 확인하고 'completed'일때까지 로그를 출력\n",
    "# status가 'completed'가 되면 루프를 탈출합니다.\n",
    "# 총 걸린 시간을 추가로 출력합니다.\n",
    "while True:\n",
    "  status = client.batches.retrieve(client.batches.list().data[0].id).status\n",
    "  print(\"time: \", time.strftime('%X', time.localtime()), \"   |    status: \", status)\n",
    "  if status == \"completed\":\n",
    "    break\n",
    "  time.sleep(60) # 60초에 한번씩 확인\n",
    "\n",
    "print(\"Total time: \", time.time() - start_time)\n",
    "\n",
    "\n",
    "# output file을 다운로드 받아서 출력\n",
    "file_response = client.files.content(client.batches.list().data[0].output_file_id).text.encode('utf-8').decode('unicode-escape')\n",
    "print(file_response)\n",
    "\n",
    "file_response[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
